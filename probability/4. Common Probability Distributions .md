# Common Probability Distributions

## 1. Introduction to Common Probability Distributions

Probability distributions are fundamental in modeling and understanding various phenomena in Natural Language Processing (NLP). This chapter explores Common Probability Distributions, focusing on Bernoulli and Binomial, Poisson, Gaussian (Normal), Exponential, and Multinomial distributions.

**Key Distributions Covered:**

- **Bernoulli and Binomial:** Models for binary outcomes and multiple trials.
- **Poisson:** Models for count data.
- **Gaussian (Normal):** Models for continuous data with a bell-shaped curve.
- **Exponential:** Models for time between events.
- **Multinomial:** Generalization of the binomial distribution for multiple categories.

## 2. Bernoulli and Binomial Distributions

### 2.1. Bernoulli Distribution

#### Definition

The **Bernoulli distribution** is a discrete probability distribution for a random variable which takes the value 1 with probability $p$ and the value 0 with probability $1 - p$.

$$
\mathbb{P}(X = 1) = p \quad \text{and} \quad \mathbb{P}(X = 0) = 1 - p
$$

**Notation:**

- $X$: Bernoulli random variable
- $p$: Probability of success (i.e., $X = 1$)

#### Properties

- **Mean (Expected Value):** $\mathbb{E}[X] = p$
- **Variance:** $\text{Var}(X) = p(1 - p)$

#### Bernoulli Distribution in NLP

**Example:**  
Consider the task of determining whether a given word in a sentence is a stopword (e.g., "the", "is"). Let $X$ be a Bernoulli random variable where:

- $X = 1$ if the word is a stopword.
- $X = 0$ otherwise.

If the probability that a word is a stopword is $p = 0.2$, then:

$$
\mathbb{P}(X = 1) = 0.2 \quad \text{and} \quad \mathbb{P}(X = 0) = 0.8
$$

#### Visualization

| $X$ | $\mathbb{P}(X = x)$ |
|-----|---------------------|
| 0   | $1 - p$             |
| 1   | $p$                 |

**Table Example:**

If $p = 0.2$:

| $X$ | $\mathbb{P}(X = x)$ |
|-----|---------------------|
| 0   | 0.8                 |
| 1   | 0.2                 |

### 2.2. Binomial Distribution

#### Definition

The **Binomial distribution** generalizes the Bernoulli distribution to multiple independent trials. It models the number of successes in $n$ independent Bernoulli trials, each with the same probability of success $p$.

$$
\mathbb{P}(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \quad \text{for} \quad k = 0, 1, 2, \dots, n
$$

**Notation:**

- $X$: Binomial random variable
- $n$: Number of trials
- $p$: Probability of success in each trial

#### Properties

- **Mean (Expected Value):** $\mathbb{E}[X] = np$
- **Variance:** $\text{Var}(X) = np(1 - p)$

#### Binomial Distribution in NLP

**Example:**  
Suppose we want to count the number of stopwords in a sentence of length $n = 10$. Each word has a probability $p = 0.2$ of being a stopword. Let $X$ be the Binomial random variable representing the number of stopwords.

$$
\mathbb{P}(X = k) = \binom{10}{k} (0.2)^k (0.8)^{10 - k} \quad \text{for} \quad k = 0, 1, \dots, 10
$$

#### Visualization

| $k$ | $\mathbb{P}(X = k)$ |
|-----|---------------------|
| 0   | $\binom{n}{0} p^0 (1-p)^n$ |
| 1   | $\binom{n}{1} p^1 (1-p)^{n-1}$ |
| $\vdots$ | $\vdots$ |
| $n$ | $\binom{n}{n} p^n (1-p)^0$ |

**Table Example:**

For $n = 3$ and $p = 0.2$:

| $k$ | $\mathbb{P}(X = k)$ |
|-----|---------------------|
| 0   | $\binom{3}{0} 0.2^0 0.8^3 = 0.512$ |
| 1   | $\binom{3}{1} 0.2^1 0.8^2 = 0.384$ |
| 2   | $\binom{3}{2} 0.2^2 0.8^1 = 0.096$ |
| 3   | $\binom{3}{3} 0.2^3 0.8^0 = 0.008$ |
| **Total** | **1.0**                 |

## 3. Poisson Distribution

### 3.1. Definition

The **Poisson distribution** is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space, given the events occur with a known constant mean rate and independently of the time since the last event.

$$
\mathbb{P}(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} \quad \text{for} \quad k = 0, 1, 2, \dots
$$

**Notation:**

- $X$: Poisson random variable
- $\lambda$: Average rate (mean number of occurrences)

### 3.2. Properties

- **Mean (Expected Value):** $\mathbb{E}[X] = \lambda$
- **Variance:** $\text{Var}(X) = \lambda$

### 3.3. Poisson Distribution in NLP

**Example:**  
Modeling the number of times a rare word like "qubit" appears in a large corpus. Suppose the average rate $\lambda = 0.05$ per document.

$$
\mathbb{P}(X = k) = \frac{0.05^k e^{-0.05}}{k!} \quad \text{for} \quad k = 0, 1, 2, \dots
$$

#### Interpretation

- $\mathbb{P}(X = 0)$: Probability that "qubit" does not appear in a document.
- $\mathbb{P}(X = 1)$: Probability that "qubit" appears once in a document.

### 3.4. Visualization

| $k$ | $\mathbb{P}(X = k)$ |
|-----|---------------------|
| 0   | $\frac{\lambda^0 e^{-\lambda}}{0!} = e^{-\lambda}$ |
| 1   | $\frac{\lambda^1 e^{-\lambda}}{1!} = \lambda e^{-\lambda}$ |
| $\vdots$ | $\vdots$ |
| $k$ | $\frac{\lambda^k e^{-\lambda}}{k!}$ |

**Table Example:**

For $\lambda = 2$:

| $k$ | $\mathbb{P}(X = k)$ |
|-----|---------------------|
| 0   | $\frac{2^0 e^{-2}}{0!} = e^{-2} \approx 0.1353$ |
| 1   | $\frac{2^1 e^{-2}}{1!} = 2e^{-2} \approx 0.2707$ |
| 2   | $\frac{2^2 e^{-2}}{2!} = 2e^{-2} \approx 0.2707$ |
| 3   | $\frac{2^3 e^{-2}}{3!} \approx 0.1804$ |
| $\vdots$ | $\vdots$ |
| **Total** | **1.0**                 |

## 4. Gaussian (Normal) Distribution

### 4.1. Definition

The **Gaussian (Normal) distribution** is a continuous probability distribution characterized by its bell-shaped curve, symmetric around the mean. It is defined by two parameters: the mean $\mu$ and the standard deviation $\sigma$.

$$
f_Y(y) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y - \mu)^2}{2\sigma^2}} \quad \text{for} \quad y \in \mathbb{R}
$$

**Notation:**

- $Y$: Gaussian random variable
- $\mu$: Mean
- $\sigma$: Standard deviation

### 4.2. Properties

- **Mean (Expected Value):** $\mathbb{E}[Y] = \mu$
- **Variance:** $\text{Var}(Y) = \sigma^2$
- **Symmetry:** The distribution is symmetric around $\mu$.
- **68-95-99.7 Rule:** Approximately 68% of data lies within $\mu \pm \sigma$, 95% within $\mu \pm 2\sigma$, and 99.7% within $\mu \pm 3\sigma$.

### 4.3. Gaussian Distribution in NLP

**Example:**  
Modeling the cosine similarity between word embeddings. Suppose the similarities follow a normal distribution with $\mu = 0.5$ and $\sigma = 0.1$.

$$
f_Y(y) = \frac{1}{\sqrt{2\pi} \times 0.1} e^{-\frac{(y - 0.5)^2}{2 \times 0.1^2}} \quad \text{for} \quad y \in \mathbb{R}
$$

#### Interpretation

- **Mean:** Average cosine similarity is 0.5.
- **Standard Deviation:** Spread of cosine similarities around the mean is 0.1.

### 4.4. Visualization

| $y$ Range          | $f_Y(y)$                 |
|--------------------|--------------------------|
| $y < \mu - 3\sigma$ | Near 0                    |
| $\mu - 3\sigma \leq y < \mu - 2\sigma$ | Increasing from 0 to peak |
| $\mu - 2\sigma \leq y < \mu - \sigma$ | High density near mean   |
| $\mu - \sigma \leq y \leq \mu + \sigma$ | Peak around mean          |
| $\mu + \sigma < y \leq \mu + 2\sigma$ | Decreasing towards tail   |
| $\mu + 2\sigma < y \leq \mu + 3\sigma$ | Low density near tail     |
| $y > \mu + 3\sigma$ | Near 0                    |

## 5. Exponential Distribution

### 5.1. Definition

The **Exponential distribution** is a continuous probability distribution often used to model the time between independent events that happen at a constant average rate.

$$
f_Y(y) = 
\begin{cases} 
\lambda e^{-\lambda y} & y \geq 0 \\
0 & y < 0 
\end{cases}
$$

**Notation:**

- $Y$: Exponential random variable
- $\lambda$: Rate parameter (inverse of the mean)

### 5.2. Properties

- **Mean (Expected Value):** $\mathbb{E}[Y] = \frac{1}{\lambda}$
- **Variance:** $\text{Var}(Y) = \frac{1}{\lambda^2}$
- **Memoryless Property:** The probability of an event occurring in the next $t$ units of time is independent of how much time has already elapsed.

### 5.3. Exponential Distribution in NLP

**Example:**  
Modeling the time between incoming queries in a chatbot. Suppose queries arrive at an average rate of $\lambda = 2$ queries per minute.

$$
f_Y(y) = 
\begin{cases} 
2 e^{-2y} & y \geq 0 \\
0 & y < 0 
\end{cases}
$$

#### Interpretation

- **Mean:** Average time between queries is $\frac{1}{2} = 0.5$ minutes.
- **Variance:** $\frac{1}{4} = 0.25$ minutes$^2$.

### 5.4. Visualization

| $y$ Range          | $f_Y(y)$        |
|--------------------|-----------------|
| $y < 0$            | 0               |
| $y \geq 0$         | $\lambda e^{-\lambda y}$ |

**Table Example:**

For $\lambda = 1$:

| $y$ | $f_Y(y)$             |
|-----|----------------------|
| 0   | $1 \times e^{-0} = 1$ |
| 1   | $1 \times e^{-1} \approx 0.3679$ |
| 2   | $1 \times e^{-2} \approx 0.1353$ |
| 3   | $1 \times e^{-3} \approx 0.0498$ |
| $\vdots$ | $\vdots$           |

## 6. Multinomial Distribution

### 6.1. Definition

The **Multinomial distribution** is a generalization of the binomial distribution to more than two categories. It models the counts of each category in $n$ independent trials, where each trial can result in one of $k$ possible outcomes with fixed probabilities.

$$
\mathbb{P}(X_1 = x_1, X_2 = x_2, \dots, X_k = x_k) = \frac{n!}{x_1! x_2! \dots x_k!} p_1^{x_1} p_2^{x_2} \dots p_k^{x_k}
$$

**Notation:**

- $X_1, X_2, \dots, X_k$: Counts of each category
- $p_1, p_2, \dots, p_k$: Probabilities of each category (sum to 1)
- $n$: Total number of trials

### 6.2. Properties

- **Mean:** $\mathbb{E}[X_i] = np_i$ for each category $i$
- **Variance:** $\text{Var}(X_i) = np_i(1 - p_i)$
- **Covariance:** $\text{Cov}(X_i, X_j) = -np_i p_j$ for $i \neq j$

### 6.3. Multinomial Distribution in NLP

**Example:**  
Modeling the distribution of parts of speech (POS) tags in a sentence. Suppose each word in a sentence can be classified as one of $k = 3$ POS tags: Noun ($p_1 = 0.5$), Verb ($p_2 = 0.3$), Adjective ($p_3 = 0.2$). Let $X_1$, $X_2$, and $X_3$ denote the counts of Nouns, Verbs, and Adjectives respectively in a sentence of length $n = 10$.

$$
\mathbb{P}(X_1 = x_1, X_2 = x_2, X_3 = x_3) = \frac{10!}{x_1! x_2! x_3!} (0.5)^{x_1} (0.3)^{x_2} (0.2)^{x_3}
$$

**Constraints:** $x_1 + x_2 + x_3 = 10$

### 6.4. Visualization

| Category | $p_i$ | $X_i$ | $\mathbb{P}(X_i = x_i)$ |
|----------|-------|-------|-------------------------|
| Noun     | 0.5   | $x_1$ | $\frac{10!}{x_1! (10 - x_1)!} 0.5^{x_1} 0.5^{10 - x_1}$ |
| Verb     | 0.3   | $x_2$ | $\frac{10!}{x_2! (10 - x_2)!} 0.3^{x_2} 0.7^{10 - x_2}$ |
| Adjective| 0.2   | $x_3$ | $\frac{10!}{x_3! (10 - x_3)!} 0.2^{x_3} 0.8^{10 - x_3}$ |

**Multinomial Table Example:**

For a small $n$, say $n = 3$:

| $X_1$ (Nouns) | $X_2$ (Verbs) | $X_3$ (Adjectives) | $\mathbb{P}(X_1, X_2, X_3)$ |
|---------------|---------------|---------------------|----------------------------|
| 0             | 0             | 3                   | $\frac{3!}{0!0!3!} 0.5^0 0.3^0 0.2^3 = 1 \times 1 \times 1 \times 0.008 = 0.008$ |
| 0             | 1             | 2                   | $\frac{3!}{0!1!2!} 0.5^0 0.3^1 0.2^2 = 3 \times 1 \times 0.3 \times 0.04 = 0.036$ |
| 0             | 2             | 1                   | $\frac{3!}{0!2!1!} 0.5^0 0.3^2 0.2^1 = 3 \times 1 \times 0.09 \times 0.2 = 0.054$ |
| 0             | 3             | 0                   | $\frac{3!}{0!3!0!} 0.5^0 0.3^3 0.2^0 = 1 \times 1 \times 0.027 \times 1 = 0.027$ |
| 1             | 0             | 2                   | $\frac{3!}{1!0!2!} 0.5^1 0.3^0 0.2^2 = 3 \times 0.5 \times 1 \times 0.04 = 0.06$ |
| $\vdots$      | $\vdots$      | $\vdots$            | $\vdots$                    |
| 3             | 0             | 0                   | $\frac{3!}{3!0!0!} 0.5^3 0.3^0 0.2^0 = 1 \times 0.125 \times 1 \times 1 = 0.125$ |
| **Total**     |               |                     | **1.0**                     |

## 7. Additional Important Information

### 7.1. Parameter Estimation

Estimating the parameters $p$ in Bernoulli and Binomial distributions, $\lambda$ in Poisson and Exponential distributions, and $\mu$ and $\sigma$ in Gaussian distributions is essential for applying these distributions to real-world NLP data. Techniques such as Maximum Likelihood Estimation (MLE) and Bayesian inference are commonly used.

### 7.2. Moment Generating Functions (MGFs)

MGFs provide a convenient way to calculate moments (expected values) of distributions. They are defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}]
$$

MGFs can be used to derive properties like mean and variance and to prove the distribution of sums of independent random variables.

### 7.3. Central Limit Theorem (CLT)

The **Central Limit Theorem** states that the sum of a large number of independent and identically distributed random variables tends toward a normal distribution, regardless of the original distribution's shape. This theorem justifies the widespread use of the Gaussian distribution in NLP for aggregated linguistic features.

### 7.4. Applications in NLP

- **Language Modeling:** Binomial and Poisson distributions can model word frequencies.
- **Text Classification:** Gaussian distributions can model feature distributions in classifiers like Naive Bayes.
- **Word Embeddings:** Gaussian distributions can model similarities between word vectors.
- **Topic Modeling:** Multinomial distributions are fundamental in models like Latent Dirichlet Allocation (LDA).

## 8. Python Implementation Examples

Implementing probability distributions in Python allows for practical application and deeper understanding. Below are Python examples for each of the discussed distributions.

### 8.1. Bernoulli and Binomial Distributions

**Scenario:**  
Modeling whether a word is a stopword using the Bernoulli distribution and counting stopwords in a sentence using the Binomial distribution.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import bernoulli, binom

# Bernoulli Distribution
p = 0.2  # Probability of a word being a stopword
x = [0, 1]
pmf_bernoulli = bernoulli.pmf(x, p)

# Binomial Distribution
n = 10  # Number of words in a sentence
k = np.arange(0, n+1)
pmf_binom = binom.pmf(k, n, p)

# Visualization
plt.figure(figsize=(12, 5))

# Bernoulli PMF
plt.subplot(1, 2, 1)
plt.stem(x, pmf_bernoulli, use_line_collection=True)
plt.title('Bernoulli PMF (p=0.2)')
plt.xlabel('Outcome (0: Not Stopword, 1: Stopword)')
plt.ylabel('Probability')
plt.xticks(x)

# Binomial PMF
plt.subplot(1, 2, 2)
plt.stem(k, pmf_binom, use_line_collection=True)
plt.title('Binomial PMF (n=10, p=0.2)')
plt.xlabel('Number of Stopwords')
plt.ylabel('Probability')
plt.xticks(k)

plt.tight_layout()
plt.show()
```

**Output:**

- **Bernoulli PMF Plot:** Shows probabilities for a word being a stopword or not.
- **Binomial PMF Plot:** Displays the distribution of the number of stopwords in a 10-word sentence.

### 8.2. Poisson Distribution

**Scenario:**  
Modeling the number of times a rare word like "qubit" appears in a large corpus using the Poisson distribution.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

# Poisson Distribution
lambda_poisson = 0.05  # Average rate
k = np.arange(0, 10)
pmf_poisson = poisson.pmf(k, lambda_poisson)

# Visualization
plt.figure(figsize=(8, 5))
plt.stem(k, pmf_poisson, use_line_collection=True)
plt.title('Poisson PMF (λ=0.05)')
plt.xlabel('Number of Occurrences')
plt.ylabel('Probability')
plt.xticks(k)
plt.show()
```

**Output:**

- **Poisson PMF Plot:** Shows the probability of "qubit" appearing 0, 1, 2, etc., times in a document.

### 8.3. Gaussian (Normal) Distribution

**Scenario:**  
Modeling the cosine similarity between word embeddings using a Gaussian distribution.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Gaussian Distribution
mu = 0.5  # Mean similarity
sigma = 0.1  # Standard deviation
y = np.linspace(0, 1, 1000)
pdf_normal = norm.pdf(y, mu, sigma)
cdf_normal = norm.cdf(y, mu, sigma)

# Visualization
plt.figure(figsize=(12, 5))

# PDF Plot
plt.subplot(1, 2, 1)
plt.plot(y, pdf_normal, color='blue')
plt.title('Normal PDF (μ=0.5, σ=0.1)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Density')
plt.grid(True)

# CDF Plot
plt.subplot(1, 2, 2)
plt.plot(y, cdf_normal, color='orange')
plt.title('Normal CDF (μ=0.5, σ=0.1)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Cumulative Probability')
plt.grid(True)

plt.tight_layout()
plt.show()
```

**Output:**

- **Normal PDF Plot:** Illustrates the density of cosine similarities around the mean.
- **Normal CDF Plot:** Shows the cumulative probability up to each cosine similarity value.

### 8.4. Exponential Distribution

**Scenario:**  
Modeling the time between incoming queries in a chatbot using the Exponential distribution.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import expon

# Exponential Distribution
lambda_exp = 2  # Rate parameter (queries per minute)
y = np.linspace(0, 3, 1000)
pdf_expon = expon.pdf(y, scale=1/lambda_exp)
cdf_expon = expon.cdf(y, scale=1/lambda_exp)

# Visualization
plt.figure(figsize=(12, 5))

# PDF Plot
plt.subplot(1, 2, 1)
plt.plot(y, pdf_expon, color='green')
plt.title('Exponential PDF (λ=2)')
plt.xlabel('Time Between Queries (minutes)')
plt.ylabel('Density')
plt.grid(True)

# CDF Plot
plt.subplot(1, 2, 2)
plt.plot(y, cdf_expon, color='red')
plt.title('Exponential CDF (λ=2)')
plt.xlabel('Time Between Queries (minutes)')
plt.ylabel('Cumulative Probability')
plt.grid(True)

plt.tight_layout()
plt.show()
```

**Output:**

- **Exponential PDF Plot:** Shows the density of times between queries.
- **Exponential CDF Plot:** Displays the cumulative probability up to each time value.

### 8.5. Multinomial Distribution

**Scenario:**  
Modeling the distribution of parts of speech (POS) tags in a sentence using the Multinomial distribution.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multinomial
import pandas as pd

# Multinomial Distribution
n = 10  # Number of words in a sentence
p = [0.5, 0.3, 0.2]  # Probabilities for Noun, Verb, Adjective
k = 3  # Number of categories

# Possible outcomes are combinations where x1 + x2 + x3 = n
# For simplicity, we'll visualize some probabilities

# Generate all possible combinations for n=3
from itertools import product

n_small = 3
combinations = [comb for comb in product(range(n_small+1), repeat=k) if sum(comb) == n_small]
probabilities = [multinomial.pmf(comb, n_small, p) for comb in combinations]

# Create DataFrame
df = pd.DataFrame(combinations, columns=['Nouns', 'Verbs', 'Adjectives'])
df['Probability'] = probabilities

print("Multinomial Distribution (n=3, p=[0.5, 0.3, 0.2]):")
print(df)

# Visualization
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(df['Nouns'], df['Verbs'], df['Adjectives'], c=df['Probability'], cmap='viridis')

ax.set_xlabel('Nouns')
ax.set_ylabel('Verbs')
ax.set_zlabel('Adjectives')
ax.set_title('Multinomial Distribution (n=3, p=[0.5, 0.3, 0.2])')

plt.show()
```

**Output:**

- **Multinomial Distribution Table:** Displays probabilities for different combinations of POS counts in a sentence of length 3.
- **3D Scatter Plot:** Visualizes the distribution of POS counts with color intensity representing probabilities.

## 9. Summary

This chapter provided an in-depth exploration of **Common Probability Distributions** essential for NLP applications. We covered:

- **Bernoulli and Binomial Distributions:**
  - **Bernoulli:** Models binary outcomes (e.g., word being a stopword).
  - **Binomial:** Extends Bernoulli to multiple trials (e.g., number of stopwords in a sentence).

- **Poisson Distribution:**
  - Models count data (e.g., occurrence of rare words).

- **Gaussian (Normal) Distribution:**
  - Models continuous data with symmetric bell curves (e.g., cosine similarity between word embeddings).

- **Exponential Distribution:**
  - Models time between events (e.g., time between chatbot queries).

- **Multinomial Distribution:**
  - Generalizes Binomial to multiple categories (e.g., distribution of POS tags in a sentence).

**Key Takeaways:**

- **Model Selection:** Choosing the appropriate distribution is crucial based on the nature of the data (binary, count, continuous, etc.).
- **Parameter Estimation:** Accurately estimating distribution parameters (e.g., $p$, $\lambda$, $\mu$, $\sigma$) is essential for effective modeling.
- **Applications in NLP:** These distributions underpin various NLP tasks, enabling probabilistic modeling of language data.
- **Python Implementation:** Practical examples demonstrate how to apply these distributions using Python libraries like `scipy.stats`, `numpy`, and `matplotlib`.

**Further Considerations:**

- **Mixture Models:** Combining multiple distributions to model complex data patterns.
- **Hidden Markov Models (HMMs):** Utilizing distributions to model sequences in NLP tasks like part-of-speech tagging.
- **Topic Models:** Employing distributions like Multinomial in models such as LDA to discover topics within text corpora.

Mastery of these probability distributions equips NLP practitioners with the tools necessary to build robust

### References

1. **Jurafsky, D., & Martin, J. H. (2023).** *Speech and Language Processing*. Pearson.
2. **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** *Deep Learning*. MIT Press.
3. **Casella, G., & Berger, R. L. (2002).** *Statistical Inference*. Cengage Learning.
4. **Papoulis, A., & Pillai, S. U. (2002).** *Probability, Random Variables, and Stochastic Processes*. McGraw-Hill.
5. **Ross, S. M. (2014).** *Introduction to Probability Models*. Academic Press.
6. **Grimmett, G., & Stirzaker, D. (2001).** *Probability and Random Processes*. Oxford University Press.
7. **Manning, C. D., & Schütze, H. (1999).** *Foundations of Statistical Natural Language Processing*. MIT Press.
