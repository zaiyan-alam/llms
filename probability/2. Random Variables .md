# Random Variables

Random variables are foundational elements in probability theory and are extensively used in Natural Language Processing (NLP) to model and analyze linguistic phenomena. This chapter explores the concept of **Random Variables**, differentiating between **Discrete and Continuous Random Variables**, and delves into their associated **Probability Mass Functions (PMFs)**, **Probability Density Functions (PDFs)**, and **Cumulative Distribution Functions (CDFs)**.

## 1. Introduction to Random Variables

### 1.1. Definition

A **random variable** is a function that assigns a numerical value to each outcome in a sample space of a random experiment.

**Notation:**
- **Random Variable:** Typically denoted by uppercase letters such as $X, Y, Z$.
- **Sample Space:** $\Omega$
- **Realizations (Values):** Lowercase letters $x, y, z$.

### 1.2. Importance in NLP

In NLP, random variables are instrumental in modeling various linguistic elements and phenomena.

**Examples in NLP:**
- **Word Counts:** Number of times a specific word appears in a document.
- **Sentence Lengths:** Number of words in a sentence.
- **Cosine Similarity:** Degree of similarity between word embeddings.

## 2. Discrete and Continuous Random Variables

### 2.1. Discrete Random Variables

**Definition:**
A **discrete random variable** takes on a countable number of distinct values.

**In NLP Context:**
Discrete random variables are prevalent in NLP applications due to the inherently countable nature of linguistic elements.

**Examples:**
- **Word Counts:** Number of times a specific word appears in a text.
- **Sentence Lengths:** Number of words in a sentence.

**Visualization:**

| $X$ (Number of "data") | Probability $\mathbb{P}(X = x)$ |
|------------------------|--------------------------------|
| 0                      | 0.5                            |
| 1                      | 0.3                            |
| 2                      | 0.15                           |
| 3                      | 0.05                           |
| **Total**              | **1.0**                        |

### 2.2. Continuous Random Variables

**Definition:**
A **continuous random variable** takes on an uncountably infinite number of possible values within a given range.

**In NLP Context:**
While less common than discrete variables, continuous random variables are used in areas such as semantic similarity and latent variable models.

**Examples:**
- **Word Embeddings:** Representing words as vectors in continuous space.
- **Cosine Similarity:** Measures similarity between word vectors, resulting in a continuous value between -1 and 1.

**Visualization:**

| Range of $Y$      | Density $f_Y(y)$ |
|-------------------|-------------------|
| $-1 \leq y < -0.5$| 0.2               |
| $-0.5 \leq y < 0$ | 0.3               |
| $0 \leq y < 0.5$  | 0.3               |
| $0.5 \leq y \leq 1$| 0.2              |
| **Total Area**    | **1.0**           |

## 3. Probability Mass Functions (PMFs) and Probability Density Functions (PDFs)

### 3.1. Probability Mass Function (PMF)

**Definition:**
A **Probability Mass Function (PMF)** assigns probabilities to each possible value of a discrete random variable.

**Properties:**
1. $\mathbb{P}(X = x) \geq 0$ for all $x$.
2. $\sum_{x} \mathbb{P}(X = x) = 1$.

**Notation:**
- $f_X(x) = \mathbb{P}(X = x)$: PMF of $X$.

**Mathematical Example:**
PMF of $X$ (number of "data"):

$$
f_X(x) = 
\begin{cases} 
0.5 & \text{if } x = 0 \\
0.3 & \text{if } x = 1 \\
0.15 & \text{if } x = 2 \\
0.05 & \text{if } x = 3 \\
0 & \text{otherwise}
\end{cases}
$$

### 3.2. Probability Density Function (PDF)

**Definition:**
A **Probability Density Function (PDF)** describes the likelihood of a continuous random variable taking on a particular value.

**Properties:**
1. $f_Y(y) \geq 0$ for all $y$.
2. $\int_{-\infty}^{\infty} f_Y(y) \, dy = 1$.

**Notation:**
- $f_Y(y)$: PDF of $Y$.

**Mathematical Example:**
PDF for cosine similarity between word embeddings:

$$
f_Y(y) = 
\begin{cases} 
\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y - \mu)^2}{2\sigma^2}} & \text{for } y \in [-1, 1] \\
0 & \text{otherwise}
\end{cases}
$$

where $\mu$ is the mean similarity and $\sigma$ is the standard deviation.

### 3.3. Mathematical Example in NLP

**Problem:**
Consider a discrete random variable $X$ representing the number of times the word "machine" appears in a document. Suppose $X$ follows a Poisson distribution with parameter $\lambda = 2$.

1. **Find $\mathbb{P}(X = 3)$.**
2. **Find $\mathbb{P}(X \leq 2)$.**

**Solution:**

The Poisson PMF is given by:

$$
f_X(x) = \frac{\lambda^x e^{-\lambda}}{x!}
$$

1. **Calculate $\mathbb{P}(X = 3)$:**

$$
f_X(3) = \frac{2^3 e^{-2}}{3!} = \frac{8 e^{-2}}{6} = \frac{4}{3} e^{-2} \approx \frac{4}{3} \times 0.1353 \approx 0.1804
$$

2. **Calculate $\mathbb{P}(X \leq 2)$:**

$$
\mathbb{P}(X \leq 2) = f_X(0) + f_X(1) + f_X(2)
$$

$$
f_X(0) = \frac{2^0 e^{-2}}{0!} = e^{-2} \approx 0.1353
$$

$$
f_X(1) = \frac{2^1 e^{-2}}{1!} = 2 e^{-2} \approx 0.2707
$$

$$
f_X(2) = \frac{2^2 e^{-2}}{2!} = \frac{4 e^{-2}}{2} = 2 e^{-2} \approx 0.2707
$$

$$
\mathbb{P}(X \leq 2) \approx 0.1353 + 0.2707 + 0.2707 = 0.6767
$$

**Interpretation in NLP:**
There is approximately a 67.67% probability that the word "machine" appears at most twice in a given document.

## 4. Cumulative Distribution Functions (CDFs)

### 4.1. Definition

A **Cumulative Distribution Function (CDF)** describes the probability that a random variable takes on a value less than or equal to a specific value.

**For Discrete Random Variables:**

$$
F_X(x) = \mathbb{P}(X \leq x) = \sum_{k \leq x} \mathbb{P}(X = k)
$$

**For Continuous Random Variables:**

$$
F_Y(y) = \mathbb{P}(Y \leq y) = \int_{-\infty}^{y} f_Y(t) \, dt
$$

### 4.2. Properties of CDFs

1. **Non-decreasing:** $F(x_1) \leq F(x_2)$ if $x_1 < x_2$.
2. **Limits:**
   - $\lim_{x \to -\infty} F(x) = 0$
   - $\lim_{x \to \infty} F(x) = 1$
3. **Right-continuous:** $F(x)$ is continuous from the right.

### 4.3. CDFs in NLP

CDFs are useful for determining probabilities over ranges of values, which is essential in various NLP tasks.

**Examples:**
- **Sentence Lengths:** Probability that a sentence has at most $k$ words.
- **Word Similarities:** Probability that the cosine similarity between word embeddings is below a certain threshold.

### 4.4. Mathematical Example in NLP

**Problem:**
Let $Y$ be a continuous random variable representing the cosine similarity between word embeddings, assumed to follow a normal distribution with mean $\mu = 0.5$ and standard deviation $\sigma = 0.1$.

1. **Find $F_Y(0.6)$, the probability that $Y \leq 0.6$.**
2. **Find $F_Y(0.4)$, the probability that $Y \leq 0.4$.**

**Solution:**

Given $Y \sim \mathcal{N}(\mu = 0.5, \sigma = 0.1)$.

1. **Calculate $F_Y(0.6)$:**

Standardize $Y$:

$$
z = \frac{0.6 - 0.5}{0.1} = 1
$$

Using standard normal tables or a calculator:

$$
F_Y(0.6) = \Phi(1) \approx 0.8413
$$

2. **Calculate $F_Y(0.4)$:**

Standardize $Y$:
[Continuation of the previous content...]

$$
z = \frac{0.4 - 0.5}{0.1} = -1
$$

Using standard normal tables or a calculator:

$$
F_Y(0.4) = \Phi(-1) \approx 0.1587
$$

**Interpretation in NLP:**
- There is an 84.13% probability that the cosine similarity between two word embeddings is at most 0.6.
- There is a 15.87% probability that the cosine similarity is at most 0.4.

## 5. Additional Important Information

### 5.1. Expected Value and Variance (Brief Introduction)

Understanding the **expected value** and **variance** of random variables is essential for grasping their behavior.

- **Expected Value ($\mathbb{E}[X]$)**: The long-run average value of repetitions of the experiment.
  
  - **Discrete:** $\mathbb{E}[X] = \sum_{x} x \cdot \mathbb{P}(X = x)$
  - **Continuous:** $\mathbb{E}[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy$

- **Variance ($\text{Var}(X)$)**: Measures the spread of the random variable's values.
  
  - **Discrete:** $\text{Var}(X) = \sum_{x} (x - \mathbb{E}[X])^2 \cdot \mathbb{P}(X = x)$
  - **Continuous:** $\text{Var}(Y) = \int_{-\infty}^{\infty} (y - \mathbb{E}[Y])^2 \cdot f_Y(y) \, dy$

**In NLP Context:**
- **Expected Value:** Average number of times a word appears across documents.
- **Variance:** Variability in word counts across documents.

### 5.2. Joint and Marginal Distributions

**Joint Distribution:**
Describes the probability distribution of two or more random variables simultaneously.

**Marginal Distribution:**
Obtained by summing or integrating out other variables from the joint distribution.

**In NLP Context:**
- **Joint Distribution:** Probability of two words appearing together in a sentence.
- **Marginal Distribution:** Probability distribution of individual words irrespective of others.

### 5.3. Independence of Random Variables

Two random variables $X$ and $Y$ are **independent** if:

$$
\mathbb{P}(X = x \text{ and } Y = y) = \mathbb{P}(X = x) \cdot \mathbb{P}(Y = y)
$$

**In NLP Context:**
Independence assumptions simplify models. For instance, in unigram models, words are assumed to be independent of each other.

### 5.4. Conditional Probability (Brief Introduction)

**Definition:**
The probability of event $A$ given that event $B$ has occurred is denoted as $\mathbb{P}(A|B)$ and is defined as:

$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$

**In NLP Context:**
In bigram models, the probability of a word given the previous word is a form of conditional probability, e.g., $\mathbb{P}(\text{"dog"}|\text{"the"})$.

## 6. Python Implementation Examples

Practical implementation reinforces the understanding of random variables. Below are Python examples illustrating discrete and continuous random variables, their PMFs/PDFs, and CDFs.

### 6.1. Discrete Random Variable: Word Count

**Scenario:**
Model the number of times the word "python" appears in a tweet using a Poisson distribution with $\lambda = 1$.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

# Define parameters
lambda_poisson = 1
x_values = np.arange(0, 6)  # Possible counts: 0 to 5

# PMF
pmf = poisson.pmf(x_values, lambda_poisson)

# CDF
cdf = poisson.cdf(x_values, lambda_poisson)

# Visualization
plt.figure(figsize=(10, 5))

# PMF plot
plt.subplot(1, 2, 1)
plt.stem(x_values, pmf, use_line_collection=True)
plt.title('Poisson PMF (λ=1)')
plt.xlabel('Number of "python"')
plt.ylabel('Probability')
plt.xticks(x_values)

# CDF plot
plt.subplot(1, 2, 2)
plt.step(x_values, cdf, where='post')
plt.title('Poisson CDF (λ=1)')
plt.xlabel('Number of "python"')
plt.ylabel('Cumulative Probability')
plt.xticks(x_values)

plt.tight_layout()
plt.show()
```

**Output:**
- **PMF Plot:** Displays the probability of the word "python" appearing 0, 1, 2, etc., times in a tweet.
- **CDF Plot:** Shows the cumulative probability up to each count.

### 6.2. Continuous Random Variable: Cosine Similarity

**Scenario:**
Assume the cosine similarity between word embeddings follows a normal distribution with $\mu = 0.5$ and $\sigma = 0.1$. Compute and plot the PDF and CDF.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Define parameters
mu = 0.5
sigma = 0.1
y_values = np.linspace(0, 1, 1000)

# PDF
pdf = norm.pdf(y_values, mu, sigma)

# CDF
cdf = norm.cdf(y_values, mu, sigma)

# Visualization
plt.figure(figsize=(12, 6))

# PDF plot
plt.subplot(1, 2, 1)
plt.plot(y_values, pdf, label='PDF')
plt.title('Normal PDF (μ=0.5, σ=0.1)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Density')
plt.legend()

# CDF plot
plt.subplot(1, 2, 2)
plt.plot(y_values, cdf, label='CDF', color='orange')
plt.title('Normal CDF (μ=0.5, σ=0.1)')
plt.xlabel('Cosine Similarity')
plt.ylabel('Cumulative Probability')
plt.legend()

plt.tight_layout()
plt.show()
```

**Output:**
- **PDF Plot:** Illustrates the density of cosine similarities around the mean.
- **CDF Plot:** Shows the cumulative probability up to each cosine similarity value.

### 6.3. Calculating Expected Value and Variance

**Scenario:**
Using the discrete random variable $X$ (number of "data" in a document) with PMF:

| $X$ | $\mathbb{P}(X = x)$ |
|-----|---------------------|
| 0   | 0.5                 |
| 1   | 0.3                 |
| 2   | 0.15                |
| 3   | 0.05                |

**Python Code:**

```python
import numpy as np

# Define the PMF
x = np.array([0, 1, 2, 3])
probabilities = np.array([0.5, 0.3, 0.15, 0.05])

# Expected Value
expected_value = np.sum(x * probabilities)
print(f"Expected Value (E[X]): {expected_value}")

# Variance
variance = np.sum((x - expected_value)**2 * probabilities)
print(f"Variance (Var[X]): {variance}")
```

**Output:**
```
Expected Value (E[X]): 0.8
Variance (Var[X]): 0.56
```

**Interpretation:**
- On average, the word "data" appears 0.8 times per document.
- The variance indicates the variability in the count of "data" across documents.

### 6.4. Joint and Marginal Distributions

**Scenario:**
Model the joint distribution of two words, "data" and "science", appearing in the same sentence.

**Assumptions:**
- $X$: Number of times "data" appears.
- $Y$: Number of times "science" appears.
- Both follow Poisson distributions with $\lambda_X = 1$ and $\lambda_Y = 1$, respectively.
- Assume independence.

**Python Code:**

```python
from scipy.stats import poisson
import numpy as np
import pandas as pd

# Define parameters
lambda_x = 1
lambda_y = 1

# Possible counts
x_counts = np.arange(0, 4)
y_counts = np.arange(0, 4)

# Initialize joint distribution table
joint_prob = np.zeros((len(x_counts), len(y_counts)))

# Calculate joint probabilities assuming independence
for i, x in enumerate(x_counts):
    for j, y in enumerate(y_counts):
        joint_prob[i, j] = poisson.pmf(x, lambda_x) * poisson.pmf(y, lambda_y)

# Display joint distribution table
joint_df = pd.DataFrame(joint_prob, index=x_counts, columns=y_counts)
joint_df.index.name = 'Data Count (X)'
joint_df.columns.name = 'Science Count (Y)'

print("Joint Probability Distribution (X and Y):")
print(joint_df)

# Calculate marginal distribution for X
marginal_x = joint_df.sum(axis=1)
print("\nMarginal Distribution for X (Data Count):")
print(marginal_x)

# Calculate marginal distribution for Y
marginal_y = joint_df.sum(axis=0)
print("\nMarginal Distribution for Y (Science Count):")
print(marginal_y)
```

**Output:**
```
Joint Probability Distribution (X and Y):
Science Count (Y)    0         1         2         3
Data Count (X)                                        
0                 0.367879 0.367879 0.183940 0.061313
1                 0.367879 0.367879 0.183940 0.061313
2                 0.183940 0.183940 0.091970 0.030654
3                 0.061313 0.061313 0.030654 0.010218

Marginal Distribution for X (Data Count):
Data Count (X)
0    0.367879
1    0.367879
2    0.183940
3    0.061313
Name: 0, dtype: float64

Marginal Distribution for Y (Science Count):
Science Count (Y)
0    0.367879
1    0.367879
2    0.183940
3    0.061313
Name: 0, dtype: float64
```

**Interpretation:**
- The **Joint Distribution Table** shows the probability of "data" appearing $x$ times and "science" appearing $y$ times in the same sentence.
- The **Marginal Distributions** confirm that $X$ and $Y$ are independent, as they match their individual Poisson distributions.

## 7. Summary

Random variables provide a systematic framework for modeling and analyzing uncertain linguistic phenomena in NLP. By distinguishing between discrete and continuous types and utilizing PMFs, PDFs, and CDFs, we can effectively represent and compute probabilities associated with various NLP tasks. 

**Key Takeaways:**

- **Random Variables ($X, Y$)**: Numerical representations of outcomes in a sample space.
  
- **Discrete vs. Continuous:**
  - **Discrete**: Countable outcomes (e.g., word counts).
  - **Continuous**: Uncountably infinite outcomes (e.g., similarity scores).
  
- **PMFs and PDFs:**
  - **PMF**: Assigns probabilities to discrete outcomes.
  - **PDF**: Describes the density of probabilities over continuous ranges.
  
- **CDFs:**
  - Represent cumulative probabilities up to a certain value.
  - Useful for range-based probability queries.
  
- **Additional Concepts:**
  - **Expected Value and Variance**: Measure central tendency and variability.
  - **Joint and Marginal Distributions**: Analyze relationships between multiple random variables.
  - **Independence**: Simplifies modeling assumptions.

Understanding these concepts is fundamental for constructing probabilistic models in NLP, such as language models, topic models, and various machine learning algorithms that underpin modern NLP systems. Mastery of random variables and their associated functions enables the development of sophisticated models capable of capturing the complexities inherent in human language.

### References
1. **Jurafsky, D., & Martin, J. H. (2023).** *Speech and Language Processing*. Pearson.
2. **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** *Deep Learning*. MIT Press.
3. **Manning, C. D., & Schütze, H. (1999).** *Foundations of Statistical Natural Language Processing*. MIT Press.
