# Chapter: Random Variables in NLP

Random variables are pivotal in bridging probability theory with real-world applications, including Natural Language Processing (NLP). They provide a way to quantify outcomes of random experiments, enabling the modeling and analysis of various linguistic phenomena. This chapter delves into **Random Variables**, distinguishing between **Discrete and Continuous Random Variables**, and explores their associated **Probability Mass Functions (PMFs)**, **Probability Density Functions (PDFs)**, and **Cumulative Distribution Functions (CDFs)**.

## 1. Introduction to Random Variables

### 1.1. Definition

A **random variable** is a function that assigns a numerical value to each outcome in a sample space of a random experiment.

**Notation:**
- **Random Variable:** Typically denoted by uppercase letters such as $X, Y, Z$.
- **Sample Space:** $\Omega$
- **Realizations (Values):** Lowercase letters $x, y, z$.

### 1.2. Importance in NLP

In NLP, random variables can represent various linguistic elements or phenomena, such as word counts, sentence lengths, or the occurrence of specific words or phrases.

**Example in NLP:**
- Let $X$ be a random variable representing the number of times the word "cat" appears in a document.

## 2. Discrete and Continuous Random Variables

### 2.1. Discrete Random Variables

**Definition:**
A **discrete random variable** takes on a countable number of distinct values.

**In NLP Context:**
- **Word Counts:** The number of times a specific word appears in a text.
- **Sentence Lengths:** The number of words in a sentence.

**Example:**
Let $X$ be the number of times the word "data" appears in a tweet. Possible values of $X$ could be $0, 1, 2, \dots$.

### 2.2. Continuous Random Variables

**Definition:**
A **continuous random variable** takes on an uncountable infinite number of possible values within a given range.

**In NLP Context:**
- **Word Embeddings:** Representing words as vectors in continuous space.
- **Latent Variable Models:** Parameters in models like Latent Dirichlet Allocation (LDA).

**Example:**
Let $Y$ represent the cosine similarity between two word embeddings. Possible values of $Y$ range between -1 and 1.

## 3. Probability Mass Functions (PMFs) and Probability Density Functions (PDFs)

### 3.1. Probability Mass Function (PMF)

**Definition:**
A **Probability Mass Function (PMF)** assigns probabilities to each possible value of a discrete random variable.

**Properties:**
1. $\mathbb{P}(X = x) \geq 0$ for all $x$.
2. $\sum_{x} \mathbb{P}(X = x) = 1$.

**Notation:**
- $f_X(x) = \mathbb{P}(X = x)$: PMF of $X$.

**Example:**
PMF for the number of times "data" appears in a tweet:

$$
f_X(x) = \begin{cases} 
0.5 & \text{if } x = 0 \\
0.3 & \text{if } x = 1 \\
0.15 & \text{if } x = 2 \\
0.05 & \text{if } x = 3 \\
0 & \text{otherwise}
\end{cases}
$$

### 3.2. Probability Density Function (PDF)

**Definition:**
A **Probability Density Function (PDF)** describes the likelihood of a continuous random variable to take on a particular value.

**Properties:**
1. $f_Y(y) \geq 0$ for all $y$.
2. $\int_{-\infty}^{\infty} f_Y(y) \, dy = 1$.

**Example:**
PDF for cosine similarity between word embeddings:

$$
f_Y(y) = \begin{cases} 
\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y - \mu)^2}{2\sigma^2}} & \text{for } y \in [-1, 1] \\
0 & \text{otherwise}
\end{cases}
$$

where $\mu$ is the mean similarity and $\sigma$ is the standard deviation.

## 4. Cumulative Distribution Functions (CDFs)

### 4.1. Definition

A **Cumulative Distribution Function (CDF)** describes the probability that a random variable takes on a value less than or equal to a specific value.

**For Discrete Random Variables:**

$$
F_X(x) = \mathbb{P}(X \leq x) = \sum_{k \leq x} \mathbb{P}(X = k)
$$

**For Continuous Random Variables:**

$$
F_Y(y) = \mathbb{P}(Y \leq y) = \int_{-\infty}^{y} f_Y(t) \, dt
$$

### 4.2. Properties of CDFs

1. **Non-decreasing:** $F(x_1) \leq F(x_2)$ if $x_1 < x_2$.
2. **Limits:**
   - $\lim_{x \to -\infty} F(x) = 0$
   - $\lim_{x \to \infty} F(x) = 1$
3. **Right-continuous:** $F(x)$ is continuous from the right.

## 5. Additional Important Information

### 5.1. Expected Value and Variance

- **Expected Value ($\mathbb{E}[X]$)**: The long-run average value of repetitions of the experiment.
  
  - **Discrete:** $\mathbb{E}[X] = \sum_{x} x \cdot \mathbb{P}(X = x)$
  - **Continuous:** $\mathbb{E}[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy$

- **Variance ($\text{Var}(X)$)**: Measures the spread of the random variable's values.
  
  - **Discrete:** $\text{Var}(X) = \sum_{x} (x - \mathbb{E}[X])^2 \cdot \mathbb{P}(X = x)$
  - **Continuous:** $\text{Var}(Y) = \int_{-\infty}^{\infty} (y - \mathbb{E}[Y])^2 \cdot f_Y(y) \, dy$

### 5.2. Independence of Random Variables

Two random variables $X$ and $Y$ are **independent** if:

$$
\mathbb{P}(X = x \text{ and } Y = y) = \mathbb{P}(X = x) \cdot \mathbb{P}(Y = y)
$$

## 6. Python Implementation Examples

For Python implementation examples, please refer to the original document. These examples include:

1. Modeling discrete random variables (e.g., word counts)
2. Modeling continuous random variables (e.g., cosine similarities)
3. Calculating expected values and variances
4. Working with joint and marginal distributions

## 7. Summary

Random variables provide a systematic way to model and analyze uncertain linguistic phenomena in NLP. Key concepts include:

- **Random Variables ($X, Y$)**: Numerical representations of outcomes in a sample space.
- **Discrete vs. Continuous**:
  - **Discrete**: Countable outcomes (e.g., word counts).
  - **Continuous**: Uncountably infinite outcomes (e.g., similarity scores).
- **PMFs and PDFs**:
  - **PMF**: Assigns probabilities to discrete outcomes.
  - **PDF**: Describes the density of probabilities over continuous ranges.
- **CDFs**: Represent cumulative probabilities up to a certain value.

Understanding these concepts is fundamental for constructing probabilistic models in NLP, such as language models, topic models, and various machine learning algorithms that underpin modern NLP systems.
