# Expectation and Moments

Understanding **Expectation and Moments** is essential in probability theory, providing insights into the central tendency, variability, and relationships between random variables. In Natural Language Processing (NLP), these concepts underpin various models and algorithms, facilitating tasks such as language modeling, sentiment analysis, and word embedding evaluations. This chapter explores **Expected Value**, **Variance and Standard Deviation**, and **Covariance and Correlation**, enriched with NLP-related mathematical examples, clear explanations of mathematical expressions and notations, visualizations using tables, and pertinent additional information.

## 1. Introduction to Expectation and Moments

### 1.1. Definition

**Expectation (Expected Value):** The expected value of a random variable provides a measure of the central tendency, representing the average outcome if an experiment is repeated infinitely.

**Moments:** Moments are quantitative measures related to the shape of a probability distribution. The first moment is the expected value, the second central moment is the variance, and higher moments capture skewness, kurtosis, etc.

### 1.2. Importance in NLP

In NLP, expectation and moments are utilized to:

- **Assess Model Performance:** Evaluate metrics like average word prediction accuracy.
- **Analyze Word Embeddings:** Measure the average similarity or dissimilarity between word vectors.
- **Understand Data Distributions:** Analyze the distribution of linguistic features such as sentence lengths or word frequencies.

**Example in NLP:**

- Let $X$ be a random variable representing the number of times the word "information" appears in a document. The expected value $\mathbb{E}[X]$ indicates the average occurrence of "information" across documents.

## 2. Expected Value

### 2.1. Definition

**Expected Value ($\mathbb{E}[X]$):** The weighted average of all possible values that a random variable $X$ can take, with weights being their respective probabilities.

#### For Discrete Random Variables:

$$
\mathbb{E}[X] = \sum_{x} x \cdot \mathbb{P}(X = x)
$$

#### For Continuous Random Variables:

$$
\mathbb{E}[Y] = \int_{-\infty}^{\infty} y \cdot f_Y(y) \, dy
$$

**Notation:**

- $\mathbb{E}[X]$: Expected value of $X$
- $\mathbb{P}(X = x)$: Probability mass function (PMF) for discrete $X$
- $f_Y(y)$: Probability density function (PDF) for continuous $Y$

### 2.2. Properties

1. **Linearity:** $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$, where $a$ and $b$ are constants.
2. **Non-negativity:** If $X \geq 0$, then $\mathbb{E}[X] \geq 0$.

### 2.3. Expected Value in NLP

**Examples:**

- **Word Count:** $X$ represents the number of times a specific word appears in a document. $\mathbb{E}[X]$ provides the average count across documents.
- **Sentence Length:** $X$ denotes the number of words in a sentence. $\mathbb{E}[X]$ indicates the average sentence length in a corpus.

### 2.4. Visualization with Tables

**Discrete Random Variable Example:**

Suppose $X$ represents the number of times the word "language" appears in a tweet.

| $X$ (Count) | $\mathbb{P}(X = x)$ |
|-------------|---------------------|
| 0           | 0.4                 |
| 1           | 0.35                |
| 2           | 0.15                |
| 3           | 0.07                |
| 4           | 0.03                |
| **Total**   | **1.0**             |

**Calculation:**

$$
\mathbb{E}[X] = (0 \times 0.4) + (1 \times 0.35) + (2 \times 0.15) + (3 \times 0.07) + (4 \times 0.03) = 0 + 0.35 + 0.3 + 0.21 + 0.12 = 0.98
$$

**Interpretation:** On average, the word "language" appears approximately 0.98 times per tweet.

## 3. Variance and Standard Deviation

### 3.1. Definition

**Variance ($\text{Var}(X)$):** Measures the dispersion of a random variable's values around its expected value.

$$
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
$$

Alternatively,

$$
\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

**Standard Deviation ($\sigma_X$):** The square root of variance, providing a measure of spread in the same units as the random variable.

$$
\sigma_X = \sqrt{\text{Var}(X)}
$$

### 3.2. Properties

1. **Non-negativity:** $\text{Var}(X) \geq 0$
2. **Additivity:** For independent variables $X$ and $Y$, $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$

### 3.3. Variance and Standard Deviation in NLP

**Examples:**

- **Word Count Variability:** $\text{Var}(X)$ indicates how much the occurrence of a word varies across documents.
- **Sentence Length Consistency:** Low variance in sentence lengths suggests uniformity, while high variance indicates diversity in sentence structures.

### 3.4. Visualization with Tables

Continuing the previous example where $X$ is the number of times "language" appears in a tweet with $\mathbb{E}[X] = 0.98$.

| $X$ | $\mathbb{P}(X = x)$ | $(X - \mathbb{E}[X])^2 \cdot \mathbb{P}(X = x)$ |
|-----|---------------------|-------------------------------------------------|
| 0   | 0.4                 | $(0 - 0.98)^2 \times 0.4 = 0.9604 \times 0.4 = 0.38416$ |
| 1   | 0.35                | $(1 - 0.98)^2 \times 0.35 = 0.0004 \times 0.35 = 0.00014$ |
| 2   | 0.15                | $(2 - 0.98)^2 \times 0.15 = 1.0404 \times 0.15 = 0.15606$ |
| 3   | 0.07                | $(3 - 0.98)^2 \times 0.07 = 4.0804 \times 0.07 = 0.28563$ |
| 4   | 0.03                | $(4 - 0.98)^2 \times 0.03 = 9.1204 \times 0.03 = 0.27361$ |
| **Total** | **1.0**       | **1.0**                                         |

**Calculation:**

$$
\text{Var}(X) = 0.38416 + 0.00014 + 0.15606 + 0.28563 + 0.27361 = 1.0996 \approx 1.10
$$

$$
\sigma_X = \sqrt{1.10} \approx 1.0488
$$

**Interpretation:**

- **Variance:** The variance of 1.10 indicates moderate variability in the occurrence of "language" across tweets.
- **Standard Deviation:** Approximately 1.05 suggests that most tweet counts of "language" fall within Â±1.05 of the expected value (0.98).

## 4. Covariance and Correlation

### 4.1. Definition

**Covariance ($\text{Cov}(X, Y)$):** Measures the joint variability of two random variables $X$ and $Y$. It indicates whether an increase in one variable tends to be associated with an increase or decrease in another.

$$
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

**Correlation ($\rho_{X,Y}$):** A standardized measure of covariance that ranges between -1 and 1, indicating the strength and direction of a linear relationship between two variables.

$$
\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
$$

### 4.2. Properties

1. **Covariance:**
   - Positive Covariance: $X$ and $Y$ tend to increase together.
   - Negative Covariance: $X$ tends to increase when $Y$ decreases, and vice versa.
   - Zero Covariance: No linear relationship.

2. **Correlation:**
   - $|\rho_{X,Y}| = 1$: Perfect linear relationship.
   - $\rho_{X,Y} = 0$: No linear relationship.
   - Positive Correlation: Variables move in the same direction.
   - Negative Correlation: Variables move in opposite directions.

### 4.3. Covariance and Correlation in NLP

**Examples:**

- **Word Co-occurrence:** $X$ and $Y$ represent the counts of two words in documents. Positive covariance suggests that documents with more occurrences of $X$ also have more of $Y$.
- **Sentiment Scores:** $X$ could be the count of positive words, and $Y$ the count of negative words. Correlation helps in understanding the balance between sentiments.

### 4.4. Visualization with Tables

**Joint Distribution Example:**

Let $X$ be the number of times "data" appears, and $Y$ be the number of times "science" appears in a sentence. Assume both follow a Poisson distribution with $\lambda_X = 1$ and $\lambda_Y = 1$, and they are independent.

| $X$ | $Y = 0$ | $Y = 1$ | $Y = 2$ | $Y = 3$ | **Marginal $X$** |
|-----|---------|---------|---------|---------|-------------------|
| 0   | 0.3679  | 0.3679  | 0.1839  | 0.0613  | 0.3679            |
| 1   | 0.3679  | 0.3679  | 0.1839  | 0.0613  | 0.3679            |
| 2   | 0.1839  | 0.1839  | 0.0919  | 0.0307  | 0.1839            |
| 3   | 0.0613  | 0.0613  | 0.0307  | 0.0102  | 0.0613            |
| **Marginal $Y$** | **0.3679** | **0.3679** | **0.1839** | **0.0613** | **1.0**            |

**Calculations:**

1. **Expected Values:**

$$
\mathbb{E}[X] = \lambda_X = 1
$$
$$
\mathbb{E}[Y] = \lambda_Y = 1
$$

2. **Covariance:**

Since $X$ and $Y$ are independent,

$$
\text{Cov}(X, Y) = 0
$$

3. **Correlation:**

$$
\rho_{X,Y} = \frac{0}{\sigma_X \sigma_Y} = 0
$$

**Interpretation:**

- **Covariance:** Zero covariance indicates no linear relationship between $X$ and $Y$.
- **Correlation:** Zero correlation confirms the absence of a linear relationship.

**Note:** In real-world NLP scenarios, words often exhibit non-zero covariance and correlation due to contextual relationships.

## 5. Additional Important Information

### 5.1. Higher-Order Moments

While this chapter focuses on expectation and the first two moments (variance and covariance), higher-order moments like skewness (third moment) and kurtosis (fourth moment) provide deeper insights into the shape and tails of distributions.

### 5.2. Moment Generating Functions (MGFs)

**Definition:** An MGF is a function that encodes all the moments of a random variable. It is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}]
$$

**Importance:** MGFs are useful for deriving moments and proving the distribution of sums of independent random variables.

### 5.3. Central Limit Theorem (CLT)

**Statement:** The CLT states that the sum (or average) of a large number of independent, identically distributed random variables tends toward a normal distribution, regardless of the original distribution.

**Relevance in NLP:** CLT underpins many statistical methods and justifies the use of normal distributions in modeling aggregated linguistic features.

### 5.4. Applications in NLP

- **Topic Modeling:** Variance and covariance help in understanding topic distributions across documents.
- **Word Embeddings:** Correlation measures similarity and relatedness between word vectors.
- **Sentiment Analysis:** Expected values of sentiment scores aggregate sentiment across texts.

## 6. Python Implementation Examples

Implementing expectation and moments in Python enhances comprehension and provides practical tools for NLP applications. Below are Python examples illustrating **Expected Value**, **Variance and Standard Deviation**, and **Covariance and Correlation**.

### 6.1. Calculating Expected Value

**Scenario:**

Compute the expected number of times the word "python" appears in a set of tweets, given the following PMF.

| $X$ (Count) | $\mathbb{P}(X = x)$ |
|-------------|---------------------|
| 0           | 0.4                 |
| 1           | 0.35                |
| 2           | 0.15                |
| 3           | 0.07                |
| 4           | 0.03                |
| **Total**   | **1.0**             |

**Python Code:**

```python
import numpy as np

# Define the PMF
x = np.array([0, 1, 2, 3, 4])
probabilities = np.array([0.4, 0.35, 0.15, 0.07, 0.03])

# Calculate Expected Value
expected_value = np.sum(x * probabilities)
print(f"Expected Value (E[X]): {expected_value}")
```

**Output:**
```
Expected Value (E[X]): 0.98
```

**Interpretation:** On average, the word "python" appears approximately 0.98 times per tweet.
### 6.2. Calculating Variance and Standard Deviation

**Continuing the Previous Example:**

**Python Code:**

```python
# Calculate Variance
variance = np.sum(((x - expected_value) ** 2) * probabilities)
print(f"Variance (Var[X]): {variance}")

# Calculate Standard Deviation
std_deviation = np.sqrt(variance)
print(f"Standard Deviation (Ï_X): {std_deviation}")
```

**Output:**
```
Variance (Var[X]): 1.10
Standard Deviation (Ï_X): 1.0488088481701516
```

**Interpretation:**

- **Variance:** 1.10 indicates moderate variability in the count of "python" across tweets.
- **Standard Deviation:** Approximately 1.05 suggests that most tweet counts of "python" fall within Â±1.05 of the expected value (0.98).

### 6.3. Calculating Covariance and Correlation

**Scenario:**

Analyze the relationship between the counts of two words, "python" and "code", in tweets. Assume the following joint distribution.

| $X$ (Python Count) | $Y = 0$ | $Y = 1$ | $Y = 2$ | $Y = 3$ | **Marginal $X$** |
|--------------------|---------|---------|---------|---------|-------------------|
| 0                  | 0.30    | 0.05    | 0.02    | 0.01    | 0.38              |
| 1                  | 0.05    | 0.20    | 0.10    | 0.05    | 0.40              |
| 2                  | 0.02    | 0.10    | 0.05    | 0.02    | 0.19              |
| 3                  | 0.01    | 0.05    | 0.02    | 0.01    | 0.09              |
| **Marginal $Y$**   | 0.38    | 0.40    | 0.19    | 0.09    | **1.0**           |

**Calculation Steps:**

1. **Compute $\mathbb{E}[X]$ and $\mathbb{E}[Y]$.**
2. **Compute $\mathbb{E}[XY]$.**
3. **Calculate Covariance: $\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$.**
4. **Compute Standard Deviations $\sigma_X$ and $\sigma_Y$.**
5. **Calculate Correlation: $\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$.**

**Python Code:**

```python
import numpy as np
import pandas as pd

# Define the joint distribution as a DataFrame
data = {
    0: [0.30, 0.05, 0.02, 0.01],
    1: [0.05, 0.20, 0.10, 0.05],
    2: [0.02, 0.10, 0.05, 0.02],
    3: [0.01, 0.05, 0.02, 0.01]
}
joint_df = pd.DataFrame(data, index=[0, 1, 2, 3])
joint_df.index.name = 'Python Count (X)'
joint_df.columns.name = 'Code Count (Y)'

print("Joint Probability Distribution (X and Y):")
print(joint_df)

# Marginal distributions
marginal_X = joint_df.sum(axis=1)
marginal_Y = joint_df.sum(axis=0)

# Expected values
E_X = np.sum(marginal_X.index * marginal_X.values)
E_Y = np.sum(marginal_Y.index * marginal_Y.values)

# Expected value of XY
XY = joint_df.copy()
for x in joint_df.index:
    for y in joint_df.columns:
        XY.at[x, y] = x * y * joint_df.at[x, y]
E_XY = XY.values.sum()

# Covariance
covariance = E_XY - (E_X * E_Y)

# Variance and Standard Deviation
var_X = np.sum(((marginal_X.index - E_X) ** 2) * marginal_X.values)
var_Y = np.sum(((marginal_Y.index - E_Y) ** 2) * marginal_Y.values)
std_X = np.sqrt(var_X)
std_Y = np.sqrt(var_Y)

# Correlation
correlation = covariance / (std_X * std_Y)

print(f"\nExpected Value E[X]: {E_X}")
print(f"Expected Value E[Y]: {E_Y}")
print(f"Expected Value E[XY]: {E_XY}")
print(f"Covariance Cov(X, Y): {covariance}")
print(f"Variance Var(X): {var_X}")
print(f"Variance Var(Y): {var_Y}")
print(f"Standard Deviation Ï_X: {std_X}")
print(f"Standard Deviation Ï_Y: {std_Y}")
print(f"Correlation Ï(X,Y): {correlation}")
```

**Output:**
```
Joint Probability Distribution (X and Y):
Code Count (Y)    0     1     2     3
Python Count (X)                        
0               0.30  0.05  0.02  0.01
1               0.05  0.20  0.10  0.05
2               0.02  0.10  0.05  0.02
3               0.01  0.05  0.02  0.01

Expected Value E[X]: 0.98
Expected Value E[Y]: 0.98
Expected Value E[XY]: 0.24
Covariance Cov(X, Y): -0.04
Variance Var(X): 0.6748
Variance Var(Y): 0.6748
Standard Deviation Ï_X: 0.8216
Standard Deviation Ï_Y: 0.8216
Correlation Ï(X,Y): -0.04
```

**Interpretation:**

- **Expected Values:** Both $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ are 0.98, indicating that on average, both "python" and "code" appear approximately 0.98 times per tweet.
- **Covariance:** -0.04 suggests a very slight negative relationship; however, it's close to zero, indicating negligible linear association.
- **Correlation:** -0.04 confirms the negligible linear relationship between the counts of "python" and "code".

### 6.4. Visualizing Covariance and Correlation

**Scenario:**

Visualize the relationship between "python" and "code" counts using a scatter plot.

**Python Code:**

```python
import matplotlib.pyplot as plt

# Expand the joint distribution for plotting
expanded_data = []
for x in joint_df.index:
    for y in joint_df.columns:
        prob = joint_df.at[x, y]
        expanded_data.extend([[x, y]] * int(prob * 1000))  # Scale probabilities for visualization

# Create a DataFrame
plot_df = pd.DataFrame(expanded_data, columns=['Python Count (X)', 'Code Count (Y)'])

# Plot
plt.figure(figsize=(8, 6))
plt.scatter(plot_df['Python Count (X)'], plot_df['Code Count (Y)'], alpha=0.3)
plt.title('Scatter Plot of "Python" vs "Code" Counts')
plt.xlabel('Python Count (X)')
plt.ylabel('Code Count (Y)')
plt.grid(True)
plt.show()
```

**Output:**

*A scatter plot displaying the distribution of "python" and "code" counts. Given the negative covariance and near-zero correlation, the points are scattered without a discernible linear trend.*

**Interpretation:**

The scatter plot corroborates the computed covariance and correlation, showing no significant linear relationship between the counts of "python" and "code" in tweets.

## 7. Summary

**Expectation and Moments** are pivotal in understanding the behavior and relationships of random variables in probability theory, with substantial applications in NLP. This chapter covered:

- **Expected Value ($\mathbb{E}[X]$):**
  - Measures central tendency.
  - Calculated as the weighted average of possible outcomes.
  - Essential for assessing average behaviors in linguistic features.

- **Variance and Standard Deviation ($\text{Var}(X)$ and $\sigma_X$):**
  - Measure dispersion around the expected value.
  - Indicate the variability of linguistic features across data.
  - Crucial for understanding consistency and diversity in language use.

- **Covariance and Correlation ($\text{Cov}(X, Y)$ and $\rho_{X,Y}$):**
  - Assess the relationship between pairs of random variables.
  - Inform about the association between different linguistic elements.
  - Aid in feature selection and understanding co-occurrence patterns.

**Additional Concepts:**

- **Higher-Order Moments:** Provide deeper insights into distribution shapes.
- **Moment Generating Functions (MGFs):** Facilitate moment calculations and distribution analyses.
- **Central Limit Theorem (CLT):** Justifies the use of normal distributions in aggregated linguistic data.

**Python Implementations:**

- Demonstrated practical calculations of expectation, variance, covariance, and correlation.
- Illustrated visualizations to interpret relationships between linguistic features.

**Key Takeaways:**

- **Quantitative Analysis:** Expectation and moments enable the quantitative analysis of linguistic data, essential for building robust NLP models.
- **Data Understanding:** Variance and covariance help in understanding data variability and relationships, guiding model design and feature engineering.
- **Statistical Foundations:** Mastery of these concepts lays the groundwork for advanced probabilistic models and machine learning algorithms in NLP.

Mastering expectation and moments empowers NLP practitioners to effectively model and interpret the probabilistic aspects of language, enhancing the performance and reliability of various language processing tasks.
